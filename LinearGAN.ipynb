{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VadBQBTcJ5uh"
      },
      "source": [
        "#Non convolutional GAN for generating MNIST images\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##introduction\n",
        "Hi everyone. In this notebook, we are going to make a GAN using dense layer which is going to be trained on the MNIST dataset."
      ],
      "metadata": {
        "id": "ph78MSzLgpYf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data preprocessing\n",
        "In here we import essential libraries and preprocess our dataset"
      ],
      "metadata": {
        "id": "2lxnvfZogaYx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Input, Dense, LeakyReLU, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.initializers import RandomNormal\n",
        "from tensorflow.keras.backend import clear_session\n",
        "from tensorflow.keras.utils import disable_interactive_logging\n",
        "\n",
        "from tensorflow.keras.datasets import mnist"
      ],
      "metadata": {
        "id": "uAPjkwVEfKBF"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load MNIST data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "print(\"x shape:\", x_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cesur1QpdNI6",
        "outputId": "c12b8887-2cc6-4a9a-b834-745614d3932d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x shape: (60000, 28, 28)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = (x_train.astype(np.float32) - 127.5)/127.5 #normalize pixel values (from -1 to +1) - this is better for relu function\n",
        "x_train = x_train.reshape(60000, 784) #turn our 2D image into a 1D list (28x28 = 784)"
      ],
      "metadata": {
        "id": "gliMDV7MdeK3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##AI model\n",
        "In here we create our model which consists of two models itself\n",
        "\n",
        "###Generator\n",
        "The generator model gets a random generated image (noisy image) and generates images similar to to the images that we want\n",
        "\n",
        "###Discriminator\n",
        "The discriminator tells us how much does the generated images resemble the images in the training dataset. We use this model to tell the generator how much error does it's generated image have.<br /><br />\n",
        "In short, the generator should do it's best to get the approval of the discriminator."
      ],
      "metadata": {
        "id": "NFsm6LnSgupI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aE6dwOndQFSr",
        "outputId": "a735e93f-4184-41ea-a131-4ec5e4ceb2cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "#The values in this code were suggested in a paper published on arxiv\n",
        "\n",
        "#the results are a little better when the dimensionality of the random vector is only 10.\n",
        "#the dimensionality has been left at 100 for consistency with other GAN implementations.\n",
        "randomDim = 100\n",
        "\n",
        "#optimizer\n",
        "adam = Adam(learning_rate=0.0002, beta_1=0.5)\n",
        "\n",
        "#generator\n",
        "generator = Sequential()\n",
        "generator.add(Dense(256, input_dim=randomDim, kernel_initializer=RandomNormal(stddev=0.02)))\n",
        "generator.add(LeakyReLU(0.2))\n",
        "generator.add(Dense(512))\n",
        "generator.add(LeakyReLU(0.2))\n",
        "generator.add(Dense(1024))\n",
        "generator.add(LeakyReLU(0.2))\n",
        "generator.add(Dense(784, activation='tanh'))\n",
        "generator.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "\n",
        "#discriminator\n",
        "discriminator = Sequential()\n",
        "discriminator.add(Dense(1024, input_dim=784, kernel_initializer=RandomNormal(stddev=0.02)))\n",
        "discriminator.add(LeakyReLU(0.2))\n",
        "discriminator.add(Dropout(0.3))\n",
        "discriminator.add(Dense(512))\n",
        "discriminator.add(LeakyReLU(0.2))\n",
        "discriminator.add(Dropout(0.3))\n",
        "discriminator.add(Dense(256))\n",
        "discriminator.add(LeakyReLU(0.2))\n",
        "discriminator.add(Dropout(0.3))\n",
        "discriminator.add(Dense(1, activation='sigmoid'))\n",
        "discriminator.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "\n",
        "#combined network\n",
        "discriminator.trainable = False\n",
        "ganInput = Input(shape=(randomDim,))\n",
        "x = generator(ganInput)\n",
        "ganOutput = discriminator(x)\n",
        "gan = Model(inputs=ganInput, outputs=ganOutput)\n",
        "gan.compile(loss='binary_crossentropy', optimizer=adam)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's add some utility functions"
      ],
      "metadata": {
        "id": "gKzOuL5yjrRf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#plot the loss from each batch\n",
        "def plotLoss(epoch):\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.plot(lossHistory[0], label='Generative loss')\n",
        "    plt.plot(lossHistory[1], label='Discriminitive loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.savefig('images/gan_loss_epoch_%d.png' % epoch)\n",
        "\n",
        "#create a wall of generated MNIST images\n",
        "def plotGeneratedImages(epoch, examples=100, dim=(10, 10), figsize=(10, 10)):\n",
        "    noise = np.random.normal(0, 1, size=[examples, randomDim])\n",
        "    generatedImages = generator.predict(noise)\n",
        "    generatedImages = generatedImages.reshape(examples, 28, 28)\n",
        "\n",
        "    plt.figure(figsize=figsize)\n",
        "    for i in range(generatedImages.shape[0]):\n",
        "        plt.subplot(dim[0], dim[1], i+1)\n",
        "        plt.imshow(generatedImages[i], interpolation='nearest', cmap='gray_r')\n",
        "        plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('images/gan_generated_image_epoch_%d.png' % epoch)\n",
        "\n",
        "#save the generator and discriminator networks (and weights) for later use\n",
        "def saveModels(epoch):\n",
        "    #we save our model in .keras format because the original .h5 has been deprecated\n",
        "    generator.save('models/gan_generator_epoch_%d.keras' % epoch)\n",
        "    discriminator.save('models/gan_discriminator_epoch_%d.keras' % epoch)"
      ],
      "metadata": {
        "id": "E4l_zlkRjIgd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And this is the train function<br />\n",
        "It trains both models, generates images and saves the model in each epoch"
      ],
      "metadata": {
        "id": "TdOLavqFjxxQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epochs=1, batchSize=128):\n",
        "    #the loss history of both the generator and the discriminator is saved in this list\n",
        "    #the generator's loss history at index 0\n",
        "    #and the discriminator's at index 1\n",
        "    lossHistory = [[], []]\n",
        "\n",
        "    batchCount = int(x_train.shape[0] / batchSize)\n",
        "    print('Epochs:', epochs)\n",
        "    print('Batch size:', batchSize)\n",
        "    print('Batches per epoch:', batchCount)\n",
        "\n",
        "    for e in range(1, epochs+1):\n",
        "        print('-'*15, 'Epoch %d' % e, '-'*15)\n",
        "        for _ in range(batchCount):\n",
        "            #get a random set of input noise and images\n",
        "            noise = np.random.normal(0, 1, size=[batchSize, randomDim])\n",
        "            imageBatch = x_train[np.random.randint(0, x_train.shape[0], size=batchSize)]\n",
        "\n",
        "            #generate fake MNIST images\n",
        "            generatedImages = generator.predict(noise)\n",
        "            # print np.shape(imageBatch), np.shape(generatedImages)\n",
        "            X = np.concatenate([imageBatch, generatedImages])\n",
        "\n",
        "            #labels for generated and real data\n",
        "            yDis = np.zeros(2*batchSize)\n",
        "            #one-sided label smoothing\n",
        "            yDis[:batchSize] = 0.9\n",
        "\n",
        "            #train discriminator\n",
        "            discriminator.trainable = True\n",
        "            dLoss = discriminator.train_on_batch(X, yDis)\n",
        "\n",
        "            #train generator\n",
        "            noise = np.random.normal(0, 1, size=[batchSize, randomDim])\n",
        "            yGen = np.ones(batchSize)\n",
        "            discriminator.trainable = False\n",
        "            gLoss = gan.train_on_batch(noise, yGen)\n",
        "\n",
        "        #store loss of most recent batch from this epoch\n",
        "        lossHistory[0].append(gLoss)\n",
        "        lossHistory[1].append(dLoss)\n",
        "\n",
        "        if e == 1 or e % 20 == 0:\n",
        "            plotGeneratedImages(e)\n",
        "            saveModels(e)\n",
        "\n",
        "    # Plot losses from every epoch\n",
        "    plotLoss(e)"
      ],
      "metadata": {
        "id": "ssb_JLyzjLV7"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's train the model"
      ],
      "metadata": {
        "id": "1zo3c2XakDDu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clear_session()\n",
        "disable_interactive_logging() #the progress bar dosn't do us any good in this project\n",
        "train(200, 256)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0kTarBgkCZY",
        "outputId": "baed12fa-7017-4747-b451-1d1f98f3d4b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 200\n",
            "Batch size: 256\n",
            "Batches per epoch: 234\n",
            "--------------- Epoch 1 ---------------\n",
            "--------------- Epoch 2 ---------------\n",
            "--------------- Epoch 3 ---------------\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}